{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabalho de Mineração de Dados\n",
    "### Por: Davi de Castro e João Pedro Righi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tentamos requisitar e usar o json diretamente mas havia um erro nele."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Havia um erro no json onde faltava uma vírgula em determinado item, \n",
    "por ser apenas um caso isolado escolhemos mudar manualmente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Caso fosse um erro recorrente poderíamos utilizar uma lógica de verificação e inserção de vírgulas faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Substituição concluída. O resultado foi salvo em saida.json.\n"
     ]
    }
   ],
   "source": [
    "#  Substituir \"\\u00e3\" por \"a\" e etc ao utilizar utf-8\n",
    "\n",
    "import json\n",
    "\n",
    "def substituir_unicode(input_file, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)    \n",
    " \n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        json.dump(data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Caminho para os arquivos de entrada e saída\n",
    "arquivo_entrada = 'padaria_trab.json'\n",
    "arquivo_saida = 'saida.json'\n",
    "\n",
    "# Chama a função para substituir o unicode\n",
    "substituir_unicode(arquivo_entrada, arquivo_saida)\n",
    "\n",
    "print(f\"Substituição concluída. O resultado foi salvo em {arquivo_saida}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Erros de formatação foram resolvidos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Agora vamos remover a chave \"compra\", uma vez que a ordem das compras não é relevante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A chave 'compra' foi removida e os acentos foram removidos. Dados salvos em 'saida_transformada.json'.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import unicodedata\n",
    "\n",
    "def remover_acentos(texto):\n",
    "    \"\"\"Remove acentos de um texto e converte para minúsculas.\"\"\"\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', texto.lower())\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "def processar_json(input_file, output_file):\n",
    "    \"\"\"Lê um JSON, remove a chave 'compra', e processa os produtos removendo acentos e convertendo para minúsculas.\"\"\"\n",
    "    # Carregar os dados do arquivo JSON\n",
    "    with open(input_file, 'r') as file:\n",
    "        compras = json.load(file)\n",
    "\n",
    "    # Processar cada compra\n",
    "    for compra in compras:\n",
    "        # Remover a chave 'compra'\n",
    "        if 'compra' in compra:\n",
    "            del compra['compra']\n",
    "        \n",
    "        # Remover acentos e converter para minúsculas os nomes dos produtos\n",
    "        compra['produtos'] = [remover_acentos(produto) for produto in compra['produtos']]\n",
    "\n",
    "    # Salvar os dados modificados em um novo arquivo JSON\n",
    "    with open(output_file, 'w') as file:\n",
    "        json.dump(compras, file, indent=4)\n",
    "\n",
    "    print(f\"A chave 'compra' foi removida e os acentos foram removidos. Dados salvos em '{output_file}'.\")\n",
    "\n",
    "# Chamar a função com os nomes dos arquivos\n",
    "processar_json('saida.json', 'saida_transformada.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Agora temos um arquivo json otimizado para a etapa de mineração de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados salvos em 'resultados_associacao.txt'.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "import re\n",
    "\n",
    "def carregar_compras(input_file):\n",
    "    \"\"\"Lê o arquivo JSON e retorna uma lista de listas com os produtos, unificando itens que contêm 'doce'.\"\"\"\n",
    "    try:\n",
    "        with open(input_file, 'r') as file:\n",
    "            compras = json.load(file)\n",
    "        \n",
    "        compras_unificadas = []\n",
    "        for compra in compras:\n",
    "            produtos_unificados = [\n",
    "                re.sub(r'\\bdoce\\b.*', 'doce', produto, flags=re.IGNORECASE) \n",
    "                for produto in compra.get('produtos', [])\n",
    "            ]\n",
    "            compras_unificadas.append(produtos_unificados)\n",
    "        \n",
    "        return compras_unificadas\n",
    "    except (json.JSONDecodeError, FileNotFoundError) as e:\n",
    "        print(f\"Erro ao carregar o arquivo JSON: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "\n",
    "def gerar_regras_associacao(input_file, min_suporte=0.05, min_confianca=0.3):\n",
    "    \"\"\"Gera regras de associação a partir de um conjunto de dados de compras.\"\"\"\n",
    "    compras = carregar_compras(input_file)\n",
    "\n",
    "    if not compras:\n",
    "        print(\"Nenhuma compra foi carregada. Verifique o arquivo de entrada.\")\n",
    "        return None, None, None\n",
    "\n",
    "    # Criar DataFrame com transações codificadas como hot encoding\n",
    "    all_products = set(produto for compra in compras for produto in compra)\n",
    "    df = pd.DataFrame([{produto: (produto in compra) for produto in all_products} for compra in compras])\n",
    "\n",
    "    # Aplicar o algoritmo Apriori para encontrar itens frequentes\n",
    "    frequent_itemsets = apriori(df, min_support=min_suporte, use_colnames=True)\n",
    "\n",
    "    # Gerar regras de associação\n",
    "    regras = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=min_confianca)\n",
    "\n",
    "    # Encontrar as 5 principais regras baseadas na confiança\n",
    "    top_5_regras = regras.nlargest(5, 'confidence')\n",
    "\n",
    "    # Encontrar a regra mais influente 1 para 1 (i.e., um produto implicando diretamente em outro)\n",
    "    regra_mais_influente = regras[(regras['antecedents'].apply(len) == 1) & (regras['consequents'].apply(len) == 1)].nlargest(1, 'confidence')\n",
    "\n",
    "    # Encontrar e ordenar regras que implicam na compra de \"doce\"\n",
    "    # Unificar variações de \"doce\" apenas para a busca de regras envolvendo \"doce\"\n",
    "    regras_para_doce = regras.copy()\n",
    "    regras_para_doce['consequents'] = regras_para_doce['consequents'].apply(\n",
    "        lambda x: {re.sub(r'\\bdoce\\b.*', 'doce', item, flags=re.IGNORECASE) for item in x}\n",
    "    )\n",
    "    regras_para_doce = regras_para_doce[regras_para_doce['consequents'].apply(lambda x: 'doce' in x)]\n",
    "    regras_para_doce = regras_para_doce.sort_values(by='confidence', ascending=False)  # Ordenar por confiança\n",
    "\n",
    "    return top_5_regras, regra_mais_influente, regras_para_doce\n",
    "\n",
    "# Chamar a função para gerar as regras\n",
    "top_5_regras, regra_mais_influente, regras_para_doce = gerar_regras_associacao('saida_transformada.json', min_suporte=0.04, min_confianca=0.4)\n",
    "\n",
    "def salvar_resultados_em_arquivo(output_file, top_5_regras, regra_mais_influente, regras_para_doce):\n",
    "    with open(output_file, 'w') as file:\n",
    "        if top_5_regras is not None:\n",
    "            file.write(\"Top 5 Regras de Associação:\\n\\n\")\n",
    "            for index, row in top_5_regras.iterrows():\n",
    "                file.write(f\"Regra {index + 1}: Se {set(row['antecedents'])} então {set(row['consequents'])}\\n\")\n",
    "                file.write(f\"  - Suporte: {row['support']:.4f}\\n\")\n",
    "                file.write(f\"  - Confiança: {row['confidence']:.4f}\\n\")\n",
    "                file.write(f\"  - Lift: {row['lift']:.4f}\\n\\n\")\n",
    "\n",
    "        file.write(\"\\nRegra Mais Influente 1 para 1:\\n\\n\")\n",
    "        if regra_mais_influente is not None and not regra_mais_influente.empty:\n",
    "            row = regra_mais_influente.iloc[0]\n",
    "            file.write(f\"Se {set(row['antecedents'])} então {set(row['consequents'])}\\n\")\n",
    "            file.write(f\"  - Suporte: {row['support']:.4f}\\n\")\n",
    "            file.write(f\"  - Confiança: {row['confidence']:.4f}\\n\")\n",
    "            file.write(f\"  - Lift: {row['lift']:.4f}\\n\\n\")\n",
    "        else:\n",
    "            file.write(\"Nenhuma regra 1 para 1 encontrada.\\n\\n\")\n",
    "\n",
    "        file.write(\"\\nRegras que Implicam na Compra de 'Doce':\\n\\n\")\n",
    "        if regras_para_doce is not None and not regras_para_doce.empty:\n",
    "            for index, row in regras_para_doce.iterrows():\n",
    "                file.write(f\"Regra {index + 1}: Se {set(row['antecedents'])} então {set(row['consequents'])}\\n\")\n",
    "                file.write(f\"  - Suporte: {row['support']:.4f}\\n\")\n",
    "                file.write(f\"  - Confiança: {row['confidence']:.4f}\\n\")\n",
    "                file.write(f\"  - Lift: {row['lift']:.4f}\\n\\n\")\n",
    "        else:\n",
    "            file.write(\"Nenhuma regra encontrada que implique na compra de 'doce'.\\n\")\n",
    "\n",
    "    print(f\"Resultados salvos em '{output_file}'.\")\n",
    "\n",
    "# Chamar a função para salvar os resultados em um arquivo\n",
    "salvar_resultados_em_arquivo('resultados_associacao.txt', top_5_regras, regra_mais_influente, regras_para_doce)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
